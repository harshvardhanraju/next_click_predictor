"""
Next Click Predictor - Optimized Cloud Run Backend with ML
Fast, lightweight ML-powered predictions optimized for Cloud Run
"""

from fastapi import FastAPI, File, UploadFile, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import os
import sys
import json
import logging
import tempfile
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional
import hashlib
import cv2
import numpy as np
from PIL import Image

# Add src directory to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

# Configure logging for Cloud Run
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize FastAPI with optimizations
app = FastAPI(
    title="Next Click Predictor API - Optimized ML",
    description="Fast ML-powered click prediction service",
    version="3.1.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS configuration for frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["GET", "POST", "OPTIONS"],
    allow_headers=["*"],
)

# Global ML components
ml_processor = None
ml_available = False

def initialize_ml_components():
    """Initialize ML components with error handling"""
    global ml_processor, ml_available
    
    try:
        # Try to import and initialize ML components
        from screenshot_processor import ScreenshotProcessor
        ml_processor = ScreenshotProcessor(use_advanced_detector=True)
        ml_available = True
        logger.info("âœ… ML components initialized successfully")
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ ML components not available: {e}")
        logger.info("ðŸ“¦ Using optimized fallback mode")
        ml_available = False
        return False

# Initialize ML on startup
initialize_ml_components()

@app.on_event("startup")
async def startup_event():
    """Startup event handler"""
    logger.info("ðŸš€ Starting Next Click Predictor Optimized ML")
    logger.info(f"ðŸ¤– ML Status: {'Available' if ml_available else 'Fallback'}")

@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "service": "Next Click Predictor Optimized ML",
        "version": "3.1.0",
        "platform": "Google Cloud Run",
        "ml_enabled": ml_available,
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "capabilities": {
            "ui_element_detection": ml_available,
            "advanced_ocr": ml_available,
            "intelligent_prediction": True,
            "fast_inference": True
        },
        "endpoints": {
            "health": "/health",
            "predict": "/predict",
            "analyze": "/analyze-screenshot",
            "docs": "/docs",
            "metrics": "/metrics"
        }
    }

@app.get("/health")
async def health_check():
    """Health check endpoint for Cloud Run monitoring"""
    return {
        "status": "healthy",
        "platform": "Google Cloud Run",
        "ml_status": "available" if ml_available else "fallback",
        "timestamp": datetime.now().isoformat(),
        "memory": "Optimized",
        "ready": True
    }

@app.post("/predict")
async def predict_next_click(
    file: UploadFile = File(..., description="Screenshot image (PNG/JPG)"),
    user_attributes: str = Form(..., description="JSON string of user attributes"),
    task_description: str = Form(..., description="User's task description")
):
    """
    Predict next click location using optimized ML or intelligent fallback
    """
    start_time = datetime.now()
    
    try:
        # Validate inputs
        await validate_inputs(file, user_attributes, task_description)
        
        # Read file content
        file_content = await file.read()
        file_size = len(file_content)
        file_hash = hashlib.md5(file_content).hexdigest()[:8]
        
        # Parse user attributes
        user_attrs = json.loads(user_attributes)
        
        logger.info(f"ðŸ” Processing prediction: {file.filename} ({file_size} bytes)")
        
        # Generate prediction
        if ml_available and ml_processor:
            prediction_result = await ml_prediction(
                file_content, file.filename, user_attrs, task_description
            )
        else:
            prediction_result = await smart_fallback_prediction(
                file_content, user_attrs, task_description, file_hash
            )
        
        # Add metadata
        processing_time = (datetime.now() - start_time).total_seconds()
        prediction_result.update({
            "processing_time": round(processing_time, 2),
            "file_hash": file_hash,
            "ml_method": "optimized_ml" if ml_available else "smart_fallback",
            "timestamp": datetime.now().isoformat()
        })
        
        logger.info(f"âœ… Prediction completed in {processing_time:.2f}s")
        return prediction_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ Prediction error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Prediction failed: {str(e)}"
        )

@app.post("/analyze-screenshot")
async def analyze_screenshot_only(
    file: UploadFile = File(..., description="Screenshot image (PNG/JPG)")
):
    """
    Analyze screenshot for UI elements (debugging endpoint)
    """
    try:
        if not file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            raise HTTPException(status_code=400, detail="Invalid file format")
        
        file_content = await file.read()
        
        if ml_available and ml_processor:
            analysis_result = await analyze_with_ml(file_content)
        else:
            analysis_result = await analyze_with_fallback(file_content)
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"Analysis error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/metrics")
async def get_metrics():
    """Get service metrics"""
    return {
        "ml_status": "available" if ml_available else "fallback",
        "capabilities": {
            "ui_detection": ml_available,
            "ocr_processing": ml_available,
            "fast_inference": True
        },
        "performance": {
            "avg_processing_time": "1-3s",
            "memory_usage": "optimized",
            "startup_time": "fast"
        },
        "system_info": {
            "python_version": sys.version.split()[0],
            "platform": "Google Cloud Run"
        },
        "timestamp": datetime.now().isoformat()
    }

async def validate_inputs(file: UploadFile, user_attributes: str, task_description: str):
    """Validate API inputs"""
    if not file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        raise HTTPException(status_code=400, detail="File must be PNG, JPG, or JPEG")
    
    try:
        json.loads(user_attributes)
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="Invalid JSON in user_attributes")
    
    if not task_description.strip():
        raise HTTPException(status_code=400, detail="Task description cannot be empty")

async def ml_prediction(
    file_content: bytes,
    filename: str,
    user_attrs: Dict[str, Any],
    task_description: str
) -> Dict[str, Any]:
    """Generate prediction using optimized ML"""
    # Save file temporarily  
    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
        temp_file.write(file_content)
        temp_path = temp_file.name
    
    try:
        logger.info("ðŸ¤– Running optimized ML prediction...")
        
        # Process screenshot
        result = ml_processor.process_screenshot(temp_path)
        
        # Get UI elements
        ui_elements = result.get('elements', [])
        
        # Find best element for task
        best_element = find_best_element_for_task(ui_elements, task_description, user_attrs)
        
        # Format result
        return format_prediction_result(ui_elements, best_element, task_description)
        
    finally:
        if os.path.exists(temp_path):
            os.unlink(temp_path)

async def smart_fallback_prediction(
    file_content: bytes,
    user_attrs: Dict[str, Any],
    task_description: str,
    file_hash: str
) -> Dict[str, Any]:
    """Smart fallback with basic image analysis"""
    try:
        # Try to get image dimensions
        nparr = np.frombuffer(file_content, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is not None:
            height, width = img.shape[:2]
            logger.info(f"ðŸ“ Image dimensions: {width}x{height}")
            
            # Simple element detection
            elements = await detect_elements_simple(img, task_description)
        else:
            width, height = 1200, 800
            elements = []
        
        # Generate intelligent prediction
        if not elements:
            elements = [generate_task_based_element(task_description, file_hash, width, height)]
        
        # Find best element
        best_element = elements[0] if elements else generate_task_based_element(task_description, file_hash, width, height)
        
        return format_prediction_result(elements, best_element, task_description)
        
    except Exception as e:
        logger.error(f"Fallback prediction error: {e}")
        # Ultimate fallback
        return generate_safe_prediction(task_description, file_hash)

async def detect_elements_simple(img: np.ndarray, task: str) -> List[Dict]:
    """Simple element detection using basic CV"""
    elements = []
    height, width = img.shape[:2]
    
    try:
        # Convert to grayscale
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Detect edges
        edges = cv2.Canny(gray, 50, 150)
        
        # Find contours (potential UI elements)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        for i, contour in enumerate(contours[:10]):  # Limit to top 10
            area = cv2.contourArea(contour)
            if 500 < area < 20000:  # Reasonable UI element size
                x, y, w, h = cv2.boundingRect(contour)
                
                # Skip elements that are too thin or too flat
                if w < 30 or h < 20 or w/h > 15 or h/w > 15:
                    continue
                
                element_type, element_text = classify_element_by_position_and_task(
                    x, y, w, h, width, height, task
                )
                
                elements.append({
                    "id": f"element_{i}",
                    "type": element_type,
                    "text": element_text,
                    "bbox": [x, y, x + w, y + h],
                    "center": [x + w//2, y + h//2],
                    "size": [w, h],
                    "prominence": calculate_element_prominence(x, y, w, h, width, height),
                    "visibility": True
                })
        
        logger.info(f"ðŸ” Detected {len(elements)} potential UI elements")
        return elements
        
    except Exception as e:
        logger.debug(f"Simple detection failed: {e}")
        return []

def classify_element_by_position_and_task(x, y, w, h, img_w, img_h, task):
    """Classify element based on position and task"""
    task_lower = task.lower()
    
    # Calculate relative position
    rel_x = x / img_w
    rel_y = y / img_h
    
    # Determine element type based on task and position
    if any(word in task_lower for word in ['login', 'sign in', 'log in']):
        if rel_y > 0.4:  # Lower part of screen
            return "button", "Sign In"
        else:
            return "form", "Email"
    elif any(word in task_lower for word in ['buy', 'purchase', 'checkout']):
        if w > 100 and h > 30:  # Button-like dimensions
            return "button", "Buy Now"
        else:
            return "form", "Quantity"
    elif any(word in task_lower for word in ['search', 'find']):
        if rel_y < 0.3:  # Upper part
            return "form", "Search"
        else:
            return "button", "Search"
    elif any(word in task_lower for word in ['continue', 'next', 'proceed']):
        return "button", "Continue"
    else:
        # Generic classification
        if w > h and w > 80:  # Wide element, likely button
            return "button", "Click Here"
        else:
            return "form", "Input"

def calculate_element_prominence(x, y, w, h, img_w, img_h):
    """Calculate element prominence score"""
    # Size factor
    area = w * h
    size_factor = min(1.0, area / (img_w * img_h) * 100)
    
    # Position factor (center is more prominent)
    center_x, center_y = x + w/2, y + h/2
    rel_x, rel_y = center_x / img_w, center_y / img_h
    center_distance = ((rel_x - 0.5)**2 + (rel_y - 0.5)**2)**0.5
    position_factor = 1.0 - center_distance
    
    # Aspect ratio factor (reasonable ratios are more prominent)
    aspect_ratio = w / h if h > 0 else 1
    aspect_factor = 1.0 if 0.2 <= aspect_ratio <= 5 else 0.5
    
    prominence = (size_factor * 0.4 + position_factor * 0.4 + aspect_factor * 0.2)
    return min(1.0, max(0.1, prominence))

def find_best_element_for_task(elements, task, user_attrs):
    """Find the best UI element for the given task"""
    if not elements:
        return None
    
    task_lower = task.lower()
    scored_elements = []
    
    for element in elements:
        score = 0.0
        element_text = element.get('text', '').lower()
        element_type = element.get('type', '').lower()
        
        # Task-based scoring
        if any(word in task_lower for word in ['login', 'sign in']):
            if 'sign' in element_text or 'login' in element_text:
                score += 0.8
            elif element_type == 'button':
                score += 0.4
        
        elif any(word in task_lower for word in ['buy', 'purchase']):
            if any(word in element_text for word in ['buy', 'purchase', 'cart']):
                score += 0.8
            elif element_type == 'button':
                score += 0.4
        
        elif any(word in task_lower for word in ['search', 'find']):
            if 'search' in element_text:
                score += 0.8
            elif element_type == 'form':
                score += 0.4
        
        # Add prominence score
        score += element.get('prominence', 0) * 0.3
        
        # User tech level adjustment
        tech_level = user_attrs.get('tech_savviness', 'medium')
        if tech_level == 'high' and element_type in ['link', 'menu']:
            score += 0.1
        elif tech_level == 'low' and element_type == 'button':
            score += 0.1
        
        scored_elements.append((element, score))
    
    # Return element with highest score
    scored_elements.sort(key=lambda x: x[1], reverse=True)
    return scored_elements[0][0] if scored_elements else elements[0]

def generate_task_based_element(task, file_hash, width, height):
    """Generate element based on task analysis"""
    task_lower = task.lower()
    
    # Determine element based on task
    if any(word in task_lower for word in ['login', 'sign in', 'log in']):
        element_type, text = "button", "Sign In"
        x, y, w, h = width//2 - 60, height//2 + 50, 120, 40
    elif any(word in task_lower for word in ['buy', 'purchase', 'checkout']):
        element_type, text = "button", "Buy Now"
        x, y, w, h = width//2 - 75, height//2, 150, 45
    elif any(word in task_lower for word in ['search', 'find']):
        element_type, text = "form", "Search"
        x, y, w, h = width//4, height//3, width//2, 40
    elif any(word in task_lower for word in ['continue', 'next']):
        element_type, text = "button", "Continue"
        x, y, w, h = width//2 - 60, height - 100, 120, 40
    else:
        element_type, text = "button", "Click Here"
        x, y, w, h = width//2 - 50, height//2, 100, 35
    
    return {
        "id": f"task_element_{file_hash}",
        "type": element_type,
        "text": text,
        "bbox": [x, y, x + w, y + h],
        "center": [x + w//2, y + h//2],
        "size": [w, h],
        "prominence": 0.8,
        "visibility": True
    }

def format_prediction_result(elements, best_element, task_description):
    """Format the prediction result"""
    if not best_element and elements:
        best_element = elements[0]
    
    if not best_element:
        best_element = {
            "id": "fallback",
            "type": "button",
            "text": "Click Here",
            "bbox": [400, 300, 520, 340],
            "center": [460, 320],
            "prominence": 0.5
        }
    
    # Format elements for frontend
    formatted_elements = []
    for elem in elements:
        bbox = elem.get('bbox', [0, 0, 100, 100])
        formatted_elements.append({
            "id": elem.get('id', 'unknown'),
            "type": elem.get('type', 'unknown'),
            "text": elem.get('text', ''),
            "x": bbox[0],
            "y": bbox[1],
            "width": bbox[2] - bbox[0],
            "height": bbox[3] - bbox[1],
            "bbox": bbox,
            "center": elem.get('center', [bbox[0] + (bbox[2]-bbox[0])//2, bbox[1] + (bbox[3]-bbox[1])//2]),
            "confidence": elem.get('prominence', 0.5),
            "prominence": elem.get('prominence', 0.5),
            "visibility": elem.get('visibility', True)
        })
    
    # Format prediction
    bbox = best_element.get('bbox', [400, 300, 520, 340])
    prediction = {
        "element_id": best_element.get('id', 'best'),
        "element_type": best_element.get('type', 'button'),
        "element_text": best_element.get('text', 'Click Here'),
        "click_probability": best_element.get('prominence', 0.7),
        "x": bbox[0],
        "y": bbox[1],
        "width": bbox[2] - bbox[0],
        "height": bbox[3] - bbox[1],
        "confidence": best_element.get('prominence', 0.7)
    }
    
    confidence_score = best_element.get('prominence', 0.7)
    
    return {
        "elements": formatted_elements,
        "prediction": prediction,
        "confidence_score": confidence_score,
        "explanation": f"Optimized ML prediction for: {task_description[:50]}...",
        "ml_metadata": {
            "total_elements": len(formatted_elements),
            "processing_method": "optimized_ml" if ml_available else "smart_fallback",
            "ui_detection": ml_available,
            "fast_inference": True
        }
    }

def generate_safe_prediction(task_description, file_hash):
    """Generate safe prediction when everything fails"""
    element = {
        "id": f"safe_{file_hash}",
        "type": "button", 
        "text": "Click Here",
        "x": 400, "y": 300, "width": 120, "height": 40,
        "bbox": [400, 300, 520, 340],
        "center": [460, 320],
        "confidence": 0.6,
        "prominence": 0.6,
        "visibility": True
    }
    
    return {
        "elements": [element],
        "prediction": {
            "element_id": element["id"],
            "element_type": element["type"],
            "element_text": element["text"],
            "click_probability": 0.6,
            "x": element["x"],
            "y": element["y"],
            "width": element["width"],
            "height": element["height"],
            "confidence": 0.6
        },
        "confidence_score": 0.6,
        "explanation": f"Safe prediction for: {task_description}",
        "ml_metadata": {
            "total_elements": 1,
            "processing_method": "safe_fallback"
        }
    }

async def analyze_with_ml(file_content: bytes):
    """Analyze screenshot with ML"""
    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
        temp_file.write(file_content)
        temp_path = temp_file.name
    
    try:
        result = ml_processor.process_screenshot(temp_path)
        return {
            "analysis_method": "optimized_ml",
            "ui_elements": result.get("elements", []),
            "total_elements": result.get("total_elements", 0),
            "screen_dimensions": result.get("screen_dimensions", []),
            "processing_metadata": result.get("processing_metadata", {}),
            "timestamp": datetime.now().isoformat()
        }
    finally:
        if os.path.exists(temp_path):
            os.unlink(temp_path)

async def analyze_with_fallback(file_content: bytes):
    """Analyze screenshot with fallback method"""
    try:
        nparr = np.frombuffer(file_content, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if img is not None:
            height, width = img.shape[:2]
            elements = await detect_elements_simple(img, "general analysis")
            
            return {
                "analysis_method": "smart_fallback",
                "ui_elements": elements,
                "total_elements": len(elements),
                "screen_dimensions": [width, height],
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "analysis_method": "minimal_fallback",
                "message": "Could not process image",
                "timestamp": datetime.now().isoformat()
            }
    except Exception as e:
        return {
            "analysis_method": "error_fallback",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.environ.get("PORT", 8080))
    
    logger.info(f"ðŸš€ Starting Optimized ML Next Click Predictor on port {port}")
    logger.info(f"ðŸ¤– ML Status: {'Available' if ml_available else 'Smart Fallback'}")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )